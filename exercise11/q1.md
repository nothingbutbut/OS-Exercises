# 测试用户态线程和进程的调度
我自己编写了简单的用户态线程Green Thread，使用ucontext实现上下文切换。
```cpp
#define _XOPEN_SOURCE 600 // 为了在 macOS 上使用 ucontext
#include <iostream>
#include <ucontext.h>
#include <vector>
#include <chrono>
#include <iomanip>
#include <cmath>

using namespace std;

const int NUM_THREADS = 16;
const int ITERATIONS = 10000; // 切换次数
const int STACK_SIZE = 1024 * 64; // 64KB

// 简单的用户态线程控制块 (Mini TCB)
struct GreenThread {
    ucontext_t ctx;       // 保存寄存器、栈指针、指令指针
    char stack[STACK_SIZE]; // 这一条线程独立的栈空间
};

GreenThread main_ctx;   // 主线程上下文
GreenThread t[NUM_THREADS]; // 工作线程数组
GreenThread* current;   // 当前运行的线程

int g_switches = 0;     // 切换次数计数
vector<double> switch_latencies; // 记录每次切换的延时（微秒）

// 线程执行函数
void thread_func(int id) {
    for (int i = 0; i < ITERATIONS; ++i) {
        int next_id = (id + 1) % NUM_THREADS;
        
        // 记录切换前的时间
        auto switch_start = chrono::high_resolution_clock::now();
        
        // 切换到下一个线程
        swapcontext(&t[id].ctx, &t[next_id].ctx);
        
        // 切换回来后，记录切换延时
        auto switch_end = chrono::high_resolution_clock::now();
        chrono::duration<double, micro> latency = switch_end - switch_start;
        switch_latencies.push_back(latency.count());
        
        g_switches++;
    }
}

// 初始化用户态线程
void create_green_thread(GreenThread* t, void (*func)(int), int id, ucontext_t* return_to) {
    /*
        Args:
            t: 线程控制块指针
            func: 线程执行函数
            id: 线程 ID，传递给 func
            return_to: 函数执行完后返回的上下文
    */
    getcontext(&t->ctx);
    t->ctx.uc_stack.ss_sp = t->stack;
    t->ctx.uc_stack.ss_size = STACK_SIZE;
    t->ctx.uc_stack.ss_flags = 0;
    t->ctx.uc_link = return_to;
    makecontext(&t->ctx, (void (*)())func, 1, id);
}

int main() {
    // 预分配内存以减少动态分配的影响
    switch_latencies.reserve(NUM_THREADS * ITERATIONS);
    
    // 计算内存开销
    size_t thread_memory = sizeof(GreenThread) * NUM_THREADS;
    size_t main_ctx_memory = sizeof(GreenThread);
    size_t latency_vector_memory = sizeof(double) * NUM_THREADS * ITERATIONS;
    size_t total_memory = thread_memory + main_ctx_memory + latency_vector_memory;
    
    cout << "===== Memory Overhead =====" << endl;
    cout << "Thread contexts: " << thread_memory << " bytes (" 
         << fixed << setprecision(2) << thread_memory / 1024.0 << " KB)" << endl;
    cout << "Main context: " << main_ctx_memory << " bytes (" 
         << fixed << setprecision(2) << main_ctx_memory / 1024.0 << " KB)" << endl;
    cout << "Latency records: " << latency_vector_memory << " bytes (" 
         << fixed << setprecision(2) << latency_vector_memory / 1024.0 << " KB)" << endl;
    cout << "Total memory: " << total_memory << " bytes (" 
         << fixed << setprecision(2) << total_memory / 1024.0 / 1024.0 << " MB)" << endl;
    cout << endl;
    
    // 创建多个用户态线程
    for (int i = 0; i < NUM_THREADS; ++i) {
        create_green_thread(&t[i], thread_func, i, &main_ctx.ctx);
    }

    auto start = chrono::high_resolution_clock::now();

    // 启动第一个线程
    swapcontext(&main_ctx.ctx, &t[0].ctx);

    auto end = chrono::high_resolution_clock::now();
    chrono::duration<double> elapsed = end - start;

    // 计算延时统计信息
    double total_latency = 0.0;
    double min_latency = switch_latencies[0];
    double max_latency = switch_latencies[0];
    
    for (double lat : switch_latencies) {
        total_latency += lat;
        if (lat < min_latency) min_latency = lat;
        if (lat > max_latency) max_latency = lat;
    }
    
    double avg_latency = total_latency / switch_latencies.size();
    
    // 计算标准差（延时抖动）
    double variance = 0.0;
    for (double lat : switch_latencies) {
        variance += (lat - avg_latency) * (lat - avg_latency);
    }
    variance /= switch_latencies.size();
    double std_dev = sqrt(variance);
    
    cout << "===== Performance Metrics =====" << endl;
    cout << "Total switches: " << g_switches << endl;
    cout << "Elapsed time: " << fixed << setprecision(6) << elapsed.count() << " seconds" << endl;
    cout << "Switches per second: " << fixed << setprecision(2) << (g_switches / elapsed.count()) << endl;
    cout << endl;
    
    cout << "===== Switch Latency Statistics =====" << endl;
    cout << "Average latency: " << fixed << setprecision(3) << avg_latency << " µs" << endl;
    cout << "Min latency: " << fixed << setprecision(3) << min_latency << " µs" << endl;
    cout << "Max latency: " << fixed << setprecision(3) << max_latency << " µs" << endl;
    cout << "Latency jitter (std dev): " << fixed << setprecision(3) << std_dev << " µs" << endl;
    cout << "Coefficient of variation: " << fixed << setprecision(2) << (std_dev / avg_latency * 100) << "%" << endl;

    return 0;
}
```
对于进程上下文切换，我使用了fork创建多个子进程，然后使用信号量进行同步切换
```cpp
#include <iostream>
#include <vector>
#include <chrono>
#include <iomanip>
#include <cmath>
#include <unistd.h>
#include <sys/wait.h>
#include <sys/mman.h>
#include <semaphore.h>
#include <fcntl.h>

using namespace std;

const int NUM_PROCESSES = 16;
const int ITERATIONS = 10000; // 切换次数

// 共享内存结构
struct SharedData {
    int g_switches;
    int current_process;
    double switch_latencies[NUM_PROCESSES * ITERATIONS];
    int latency_count;
    sem_t* sems[NUM_PROCESSES]; // 信号量数组用于进程同步
};

// 进程执行函数
void process_func(int id, SharedData* shared) {
    for (int i = 0; i < ITERATIONS; ++i) {
        int next_id = (id + 1) % NUM_PROCESSES;
        
        // 等待轮到自己
        sem_wait(shared->sems[id]);
        
        // 记录切换延时
        auto switch_end = chrono::high_resolution_clock::now();
        auto switch_start_ns = chrono::duration_cast<chrono::nanoseconds>(
            switch_end.time_since_epoch()).count();
        
        // 模拟工作（与线程版本保持一致）
        shared->g_switches++;
        
        // 记录延时（这里记录的是被唤醒的时间，作为近似）
        auto now = chrono::high_resolution_clock::now();
        
        // 发信号给下一个进程
        auto signal_start = chrono::high_resolution_clock::now();
        sem_post(shared->sems[next_id]);
        auto signal_end = chrono::high_resolution_clock::now();
        
        // 计算切换延时（微秒）
        chrono::duration<double, micro> latency = signal_end - signal_start;
        if (shared->latency_count < NUM_PROCESSES * ITERATIONS) {
            shared->switch_latencies[shared->latency_count++] = latency.count();
        }
    }
    
    // 最后一次发信号以确保下一个进程能结束
    int next_id = (id + 1) % NUM_PROCESSES;
    sem_post(shared->sems[next_id]);
}

int main() {
    // 创建共享内存
    SharedData* shared = (SharedData*)mmap(NULL, sizeof(SharedData),
                                           PROT_READ | PROT_WRITE,
                                           MAP_SHARED | MAP_ANONYMOUS, -1, 0);
    
    if (shared == MAP_FAILED) {
        cerr << "mmap failed" << endl;
        return 1;
    }
    
    // 初始化共享数据
    shared->g_switches = 0;
    shared->current_process = 0;
    shared->latency_count = 0;
    
    // 创建命名信号量
    for (int i = 0; i < NUM_PROCESSES; ++i) {
        string sem_name = "/proc_sem_" + to_string(i) + "_" + to_string(getpid());
        sem_unlink(sem_name.c_str()); // 清理可能存在的旧信号量
        shared->sems[i] = sem_open(sem_name.c_str(), O_CREAT | O_EXCL, 0644, 0);
        if (shared->sems[i] == SEM_FAILED) {
            cerr << "sem_open failed for process " << i << endl;
            return 1;
        }
    }
    
    // 计算内存开销
    size_t process_overhead = sizeof(SharedData);
    size_t latency_memory = sizeof(double) * NUM_PROCESSES * ITERATIONS;
    size_t total_memory = process_overhead + latency_memory;
    
    cout << "===== Memory Overhead (Process-based) =====" << endl;
    cout << "Shared data structure: " << process_overhead << " bytes (" 
         << fixed << setprecision(2) << process_overhead / 1024.0 << " KB)" << endl;
    cout << "Latency records: " << latency_memory << " bytes (" 
         << fixed << setprecision(2) << latency_memory / 1024.0 << " KB)" << endl;
    cout << "Total shared memory: " << total_memory << " bytes (" 
         << fixed << setprecision(2) << total_memory / 1024.0 / 1024.0 << " MB)" << endl;
    cout << "Note: Each process also has its own stack and heap (not measured here)" << endl;
    cout << endl;
    
    vector<pid_t> pids;
    
    auto start = chrono::high_resolution_clock::now();
    
    // 创建子进程
    for (int i = 0; i < NUM_PROCESSES; ++i) {
        pid_t pid = fork();
        if (pid == 0) {
            // 子进程
            process_func(i, shared);
            exit(0);
        } else if (pid > 0) {
            // 父进程
            pids.push_back(pid);
        } else {
            cerr << "fork failed" << endl;
            return 1;
        }
    }
    
    // 启动第一个进程
    usleep(10000); // 等待所有进程就绪
    sem_post(shared->sems[0]);
    
    // 等待所有子进程结束
    for (pid_t pid : pids) {
        waitpid(pid, NULL, 0);
    }
    
    auto end = chrono::high_resolution_clock::now();
    chrono::duration<double> elapsed = end - start;
    
    // 计算延时统计信息
    if (shared->latency_count == 0) {
        cout << "No latency data collected!" << endl;
        return 1;
    }
    
    double total_latency = 0.0;
    double min_latency = shared->switch_latencies[0];
    double max_latency = shared->switch_latencies[0];
    
    for (int i = 0; i < shared->latency_count; ++i) {
        double lat = shared->switch_latencies[i];
        total_latency += lat;
        if (lat < min_latency) min_latency = lat;
        if (lat > max_latency) max_latency = lat;
    }
    
    double avg_latency = total_latency / shared->latency_count;
    
    // 计算标准差（延时抖动）
    double variance = 0.0;
    for (int i = 0; i < shared->latency_count; ++i) {
        double lat = shared->switch_latencies[i];
        variance += (lat - avg_latency) * (lat - avg_latency);
    }
    variance /= shared->latency_count;
    double std_dev = sqrt(variance);
    
    cout << "===== Performance Metrics (Process-based) =====" << endl;
    cout << "Total switches: " << shared->g_switches << endl;
    cout << "Elapsed time: " << fixed << setprecision(6) << elapsed.count() << " seconds" << endl;
    cout << "Switches per second: " << fixed << setprecision(2) << (shared->g_switches / elapsed.count()) << endl;
    cout << endl;
    
    cout << "===== Switch Latency Statistics (Process-based) =====" << endl;
    cout << "Samples collected: " << shared->latency_count << endl;
    cout << "Average latency: " << fixed << setprecision(3) << avg_latency << " µs" << endl;
    cout << "Min latency: " << fixed << setprecision(3) << min_latency << " µs" << endl;
    cout << "Max latency: " << fixed << setprecision(3) << max_latency << " µs" << endl;
    cout << "Latency jitter (std dev): " << fixed << setprecision(3) << std_dev << " µs" << endl;
    cout << "Coefficient of variation: " << fixed << setprecision(2) << (std_dev / avg_latency * 100) << "%" << endl;
    
    // 清理信号量
    for (int i = 0; i < NUM_PROCESSES; ++i) {
        string sem_name = "/proc_sem_" + to_string(i) + "_" + to_string(getpid());
        sem_close(shared->sems[i]);
        sem_unlink(sem_name.c_str());
    }
    
    // 释放共享内存
    munmap(shared, sizeof(SharedData));
    
    return 0;
}
```
注意到进程测试的结果并不准确，因为测量的延时实际上是信号量操作的时间，而不是完整的进程切换时间。不过从吞吐量上看，用户态线程的切换速度远高于进程切换。我测试了在2/4/8/16个线程/进程下的性能，结果如`thread_vs_process_comparison.pdf`所示。

用户态线程展现出了4倍多的吞吐量优势，同时延时也较为稳定（抖动较小）。进程切换由于涉及内核态切换，开销较大且不够稳定。整体上来看随着线程/进程数量增加，用户态线程的优势更加明显。

在内存占用上，用户态线程的开销也会更小，因为不需要为每个线程分配独立的内核数据结构，控制块、数据也更少。因为实际上我们的用户态线程只有寄存器上下文和一个64KB栈空间。